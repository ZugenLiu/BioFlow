Even if the method described in the previous part allows us to gain into the precise mechanisms by which the already existent secondary effects of drugs are exerted. However our goal is to be able to predict wherther or not a new drug in development is likely to have a secondary effect or if an approved drug can be repositionned due to it's therapeutic effect against a different disease. Even if the models for drug toxicity mechanism developed in the previos model can help to better understand off-target effect, they cannot be used alone to estimate the probability of drug having a secondary effect, due to their qualitative nature. In fact, in order to be able to predict secondary effects, we will need to estimate how much is each of the mechanisms contributing to a secondary effect is affected and based on this decide if the mechanism is sufficiently engaged to lead to a secondary effect. In addition to that, if we are willing to repurpouse drugs for action against human pathogens, there might be an insufficient number of drugs to establish at least one model of drug activity against pathogen, much less cover all the possible mechanisms of action against pathogens. In addition to that, pathogens are frequently less studied then the model organisms (see Fig.10 for the comparison of cummulative informativities of different proteins in SwissProt).

In order to mediate those shortcomings, an method specifically aimed at predicting the off-target effects of a drug was developed, once again based on [Missiuro2009], but this time much closer to the original method. The idea is to compute the information circulation that is specific to the GO terms that have been found to be important for a secondary effet in the previous part and to calculate how much is the information flow is perturbed by the off-target effect of the drug we are interested in. In addition, we can also calculate the global and organ-specific information circulation (information circulation among proteins that are differentially expressed in a given organ), which would allow us to predict a general lethality or an organ-specific toxicity with an undefined mechanism. In addition to that, provided that protein-protein interaction networks and metabolic reaction networks differ a lot between spiecies, this method will also enables us to understand if the secondary effect expected in human could be seen in mice models or if a different organism would be required. In fact, provided that a different topology of interactome, proteins that are essential for a given function in humans migh be less important in mice, even if the proteins of interest have been humanised in it. Thus, even if they are inhibited in exactly the same fashion as in humans, mice show no secondary effect that will be appearant in humans.

In case of an drug repositionning against a human pathogen, we can compute the information circulation based on the experimentally or computationally retrieved metabolome/interactome for this organism and verify how much a drug would perturbate the overall informativity flow. If this perturbation is important, it is likely that a drug will perturbate the pathogen metabolism and regulatory network sufficiently to inhibit it's growth or even cause it's death. This method can be further enhanced to take in account the possibility of pathogen developping a resistance ot drugs. In order to do this, rather then using one single metabolic/regulational model of a pathogen, several different models representing the adaptation confering drug resistnace that are likely to emerge. If a drug or a combination of drugs manages to inhibit it all. [Kinnings2009] work on antibiotics-resistant tuberculosis treatment with repositionned medecine Comtan can represent a great basis for testing such an approach.

Following the database examination described in the previous chapter, Reactome.org was selected as the backbone database to support the information circulation computation. Provided that Reactome.org supports export of human-specific pathways to the BioPax format (see [BioPax2010] for more details), and provided the good documentation fo the terms the parsing of the database was expected to be rather simple.

In fact, BioPax specifies and describes all the possible physical entities that can be taken in account, and interactions or relations between them. There are five types of physical entities in the BioPax level 3 ontology: Protein, Dna, Rna, SmallMolecule and Complex, which may contain all of them. Dna and Rna, but not Protein types contain 'Region' subclasses. There is also support for Cellular Localisation description, Tissue/Organ specification, post-translational modification and various fragements / points of interaction with other Physical Entities. In BioPax, there are two types of regulation: Control (catalysis of biochemical reactions, control of catalysis and transcription regulation) and Conversion (Biochemical Reaction, Complex assembly, Polymearisation and Transport reaction (change of cellular localization)). BioPax equally posesses several entity formats that are present mainly for compatibility reasons, such as 'Genes' or 'Pathways' and diverse utility classes that allow to store cross-references towards different ressources, evidence types or related publications. 

Provided the detailednesss of the BioPax format, and the sufficient redoundance of attributes allowing it to encode same information through different objects and relations, an internal relationship schema was designed to store the information deemed relevant. Provided that initially this database was meant to be re-used for the study of protein evolution in the systems biology context, some of the properties casted into the database are important for the evolution investigation, somehow for the study of the impact of SNPs on the disease and drug efficiency and very little for the pure drug off-target effect prediction.

All the stable physical entities were referenced as particular cases of a Meta Entity. There are five types of Meta Entities, the same as the types of physical entities in BioPax: Dna, Rna, Protein, Complex, SmallMolecule. Complexes can group severl MetaEntities, whereas DNA corresponding to a gene, corresponding mRNA and corresponding protein are all linked together through biochemical reactions (translation and transcriptiom). Alternative splicing is accounted for by the fact that the same DNA fragment can be linked to several different mRNAs and thus different proteins, which allows to account for the co-factors responsible for the splicing variants modification. 

Meta Entity, can be modified by instantiation objects. Among the instantiation objects, the retained ones were: 
	- localization. Localization is critical for defining physical entity function, given a very different structure of relations between compounds in differnt cellular locations. For instance, beta-catenin activity is very different depending if it is encountered in the cytosol or in the cell nucleus.
	- post-translational modifications, such as phosphorilation are also critical for the protein function. For instance, Cdk1 is unable to initiate a G1/S transition without a specific specific phosphorilation pattern. Proteins responsile for this phosphorilation control of Cdk1 (over 75), are among the most critical proteins for the cell survival and their informativity can only be revealed through their role in post-translational modifications. In addition, recent publication by Sarah Teichmann [ref here] have shown that modfication of phosphorilation and methylation sites are critical for undestanding the evolution of protein complexes.
	- mutations. Especially important in the context of the evolutionary theory, they are also interesting from the point of view of personalized medecine. In fact, by projecting patient's SNPs on the 3D structure of proteins and corresponding complexes structure and checking if the mutation perturbs any of the protein features critical to it's function within metabolome or reactome. Due to the difficulties with projecting the 3D models on the Uniprot, this part have not been implemented. In order to efficiently use it, a UNIPROT-PDB mapping have to be loaded by the systems of the requests from the RSCB PDB website.

When an instantiation is applied to a Meta Entity, this Meta Entity becomes an Instatiated Entity, which inherits all the properties from it's parent meta entity, plus has the properties that are conferred by the instantiation objects. Instantiated objects are considered as a most precise level of description of molecular objects relevant to biology and are used in the information circulation computation as frequently as possible. Instiations due to localization and post-translational modifications are linked together by a common reference to the meta-object and an instantiation modification reaction, whereas instantiation due to the mutation

Fianlly a fragment class is introduced to specify the points of interest on different Physical Entities. They allow for instance to localize a specific post-translational modification, mutation, point of contact between two proteins or a catalytical site.

There are only two ways for Physical entities to interact. First, Meta Entities are able to participate to Meta Reactions, whereas Instantiated Entities participate to the Instantiated Reactions, that basically implement the Meta-Reaction as long as they weren't rendred impossible due to an instantiation event (one of the reagents is not present in the cell compartement anymore). All the reactions are treated identically, as biochemical reactions. Every reaction can and can be modulated by a Physical Entity (catalysis and inhibition are treated similarly). Second, direct protein-protein interaction, such as physical contact can be referenced too. As in case with common reaction, the interactions are casted either entirely among the instantiated entities or entirely among the meta entities.

In addition to the entity and interaction nodes, Annotations Nodes have been introduced in the database structure. Their main purpouse is to carry the information not encoded in the backbone databases, but allowing to interface them with external databases, perform indexed search orretrieve information that can be interepreted by human experts. Annotation Nodes can equally contain structured annotation, for instace GO terms or Pathway annotation and links among them. A good representation of the. A visual representation of the database can be found in the [Figure ??] and the actual class declaration can be found in the neo4j_Declarations.neo4j_type_decs_new.py

>Back to Reactome.org:

Provided that BioPax format is quite permissive, before parsing reactome a pre-parse was executed to see the links existing between different classes of objects. Full parsing report is availabe in the Annex IV: Reactome.org pre-parse. In addition to revealing the structure of Reactome database for the human organism, it also brought to surface several problems. 

First, the presence of generic Physical Entities. Some of the Physical Entities and their subclasses, such as Complex, Proteins or Small Molecules have "memberPhysicalEntity" which means that they don't represent a single physical entity, but rather a group of physical entities and will represent all of them in reactions where they are treated equivalently. In case of an unponderated information circulation calculation, this means that for big groups, the information flow through the entities that participate in the same reaction as the genetric entity or interact with it will have a disproportionately high informativity. In order to distinguish them from the individual nodes, for each Meta Entity type (complex, protein, etc..) a type_Collection was introduced (for instance, Complex_Collection). However this didn't solve the problem completely. In case of the ollfactile receptors group (300 members, most cross-linked to UNIPROT), the informativity of a specific G-coupled receptor was very high only due to the fact that this groups was so large and that each of it's elements was treated as a source of information critical for the cell survival at the same title as cell-cycle regulating kinases. Fortunately, there are only two groups like this: olfactile receptors and Zinc finger family.

Second, a closer examination on individual instnaces of the Protein class have revealed that Proteins were in fact a uniform mix of actual proteins, protein fragments and protein domain, such as alpha chains of specific proteins. Some of the Protein_Collection objects represented not collections of proteins, but rather collections of protein domains, such as all the alpha chain regions in all the proteins within the database.

Third, some instances lack of information essential for their definition (“headless Horseman” problem): for instace several post-translational modifications are provided without location (we don't know where it is), without type (we don't know what it is) and even without both (we know nor what or where it is).

Fourth, Reactome has lots of compounds "floating in the middle of nowhere", i.e pointed out only by a single reaction, single entity group or a single complex and not encountered anywhere else. There are over 6000  scuh compounds (Proteins, Complexes or PhysicalEntities) that are pointed towards only by only one unique reaction. In addition to that, the reactome.org metabolic graph has several connex components separated from the main one and exceeding three compounds. In all, Reactome has 60 connex compounds, with 3 of them having over 50 nodes. This is impossible in real-life biological systems  and indicates that our knowledge of human metabolome and interactome is still far from being complete.

Fifth, with less then 130 Dna fragments and ~50 RNAs, mostly mRNA, Reactome has almost no coverage for the mechanisms related to the gene transcription and translation, making it impossible to detect any toxicity related to gene transcription / translation or gene expression regulation. An additional database have to be find and integrated to adress such issues specifically. 

Finally, Reactome havesome issues with excessive use of comments, leading to some very relevant information being completely unavailable for computational methods. Most illustrative case is the SRC kinase. One of the most central kinases in the human oganism, recieving input from the immune system, growth/proliferation equilibrium signals, embryonic development, it's only role indicated in Rectome.org is to regulate the action of ERBB for the cell cycle, without any external output. In terms of reactome, this central protein is a dead-end, a mere sensor.  [Pictures of Reactome.org database]. The Reader can do his own investigation by going to the website and entering the name of the protein, then looking at the reactions where SRC protein is implicated.

Provided the issues with the SRC kinase, and it's lack of connexity, the data from reactome.org was complemented with a highly curated HiNt protein-protein interaction dataset [citation by Yue Lab], that is rather good at balancing this kind of shortcomings (25 individual interaction with other proteins for the SRC kinase, in all 6000 more interaction for the 2000 of the 5000 proteins cross-linked to the UNIPROT accession numbers from the Reactome database)

> TECHNICAL issues

Provided the size of the Reactome database, it could not be fitted in a system of in-memory python or java object (Python xml.Etree object representing a the parsed xml file exceeded 1 Gb, whereas the sytem of python dictionaries representing the reformatted relevant subset of the Reactome.org exceeded 500 Mb). Provided that the algorithm was run on an individual station, both could not be kept in the 2Gb RAM at the same time as RAM required for the OS (500 Mb) and the IDE-specific JVM (~300 Mb). In addition to that, any information contained in the RAM is highly volatile, and each time we would like to test new modifications added to the source folders, a full re-parse would be required. 

The possibility of using a system of SQL database to store the relations from the Reactome was considered. However any queries supproting a graph traversals would require multiple joins between relatively large tables, which is a computationally expensive task (in fact the most computationally expensive task on the SQL databases). Most free SQL databases providing no support for recursive quiries, a join would have te be done each time an relation edge between nodes would be traversed, which could rapidly rendre the task of pulling up such a database prohibitive. Finally, The implementation being executed to work on one single computer, a simple SQL backend, such as SQLite db on disk would be quite unefficient from the point of view of access times, especially for queries covering a large proportion of nodes contained in the database, where the buffering of the most popular nodes on RAM has no significant effect. Such queries being important for pulling of a relevant subset of nodes and edges to create an information conductivity matrix, required to compute the information circulation, we had to eliminate the option of using a SQL database.

Another possibility was to use RDF Stores (such as Apache Jena or SparkleDB) and operate with SPARQL queries. However, most of RDF stores have no Python bindings and are currently purely documented or are still in development. For the ease of debugging, an easy way to navigate the relationships graph was wanted, which seemed difficult to achieve with RDF stores.

Instead, a neo4j graph database have been chosen. Providing a native support for traversals and native python bindings, it also provides a friendly server database interface, allowing to easily visualise the graph [see figure ## ] sand detect anomalies due to bugs in code or backbone database. Finally, it is free, open-source, has an extensive documentation and a great support. In order to avoid a lock-in within neo4j database and to prepare the possible incorporation of several organisms or patient-specific interaction graphs, an interface layer python module "bulbs" was used. Using a bluprints/pipes/frames interfaces of neo4j, which are common to all the graph databases instead of native neo4j python bindings, it allows an easy migration to any other graph database and most notably towards Aurelius Titan DB (equally free and opensource, scaling well into hundreds of bilions of nodes and relations). In addition, bulbs supports an explicit dype declaration, which is interesting for the maintanability of the project neo4j database is available for download from neo4j.org, whereas bulbs is obtainable via pips/easy_install/Pypi and is extensively documented on bulbflow.org.

Provided that the final number of physical entities taken into account for the information flow computation is well over 25000, numpy matrixes could not be used for the flow computation. Instead a scipy.sparse module has to be used and a scikit.sparse module had to be installed to perfrom LU decomposition of the matrix, required for the computation of the information flow.

In addition to that, a MongoDB server had to be setup to store GO-specfic information circulation values. Provided the number of describing the targets present in the Reactome.org database (over 5 000), it was decided to use a specific key-value store rather then store the results of computation as plaintext files, provided that I/O operations with OS on folders with such large number of files are generally slow.

> Matrix Loading and information computation

The informativity conductance matrix is assembled by first retrieving all the physical entities participating in all the reactions, then propagating starting from them according to the physical interaction (either indicated directly or deduced by a co-participation in a complex) and appartenance to generic sets. all the connex sets of physical entities and relations between them are retrieved, all the physical entities that are not in the biggest connex set are eliminated.

In addition to that, the nodes corresponding to the small molecules that are likely to participate in lots of reactions, such as ATP/ADP or H20 are eliminated to avoind the information circulation overload through them. The are retrieved as the nodes that contribute the most to the eigenvectors corresponding to the eighenvalue with the biggest absolute value. Even if the eigenvalue approach is slightly less intuitive then the in/out vertex number based appraoch, it is still easy to understand and yields better result. If the reader is acquainted with the linear algebra basics, he will probably know that the largest eigenvalue corresponds to the approximative increase in matrix value after a sufficiently big number of multuplication by itself (i.e. raising it to a sufficiently high power). If we interpret it from the information circulation point fo view (in a manner similar to a random walk appraoch presented in the random matrix clustering), it means that if we broadcast information from each node and then on each following turn broadcast to all the nodes it is connected to the information it recieved during the previous round by ponderating it by the informativity conductance that links it to the other nodes, after a sufficient number of brioadcasets, the information distribution will become stable and the information in each node will be equal to the coefficient of it had as the eigenvector of the biggest eigenvalue (this is in fact eigenvectors clustering).

Such elimination means that we won't see a toxicity related to the change of Cell pH, eneregetical startivng. However mechanisms regulating these processes are pretty well studied and should be seen in the GO terms annotation.

>
Random matrix theory goes here. 
  - verification that that lots of the eigenvalues are actually non-random

	Not implemented => not explicited, evan if this is a very interesting topic.
>

The each type of relation gets an information conductivity attributed to it, depending on the type of relation and the database from which the relation was retrieved. In the implementation deposited on the GitHub, physical entities participating in the same reaction have an information conductivity of 0.5, whereas a direct contact interaction or co-participation in a complex is equivalent to a conductivity of 0.33. At the same time, belonging to the same group is equivalent in terms of information conductivity to only 0.10, so that the information circulation due between the elements of the same group that is due only to them belonging to the same group is highly discouraged.

For the interpretation purpouses, the information circulation is calculated only by using as sources or sinks the nodes that are cross-referenced with the SwissProt. In addition, the method described in [Missiuro2009] have been slightly improved in order to accelerate the computation. For instance, instead of computing the LU decomposition of a matrix N, a more rapid Cholesky decomposition of the matrix M+%epsilon*Id is computed, where %epsilon<10^-9. This introduces an overall error of less then 10^-7 for the values of the information circulation, but rendrers a non-nulipotent matrix ad renders the circulation matrix postive definit, allowing to use a much more rapid and reliable Cholesky decomposition rather then LU decomposition. A second improvement consists in the definition of sink. Instead of annulating the lines and columns of matrix corresponding to the current sink in the computation, the current used for the computation of the voltages in different nodes is set equal not to J=di where i=#of row/column corresponding to the source, but rather J=di+dj, where i is still the source and j is the sink. This way only one Cholesky decomposition is required for the whole matrix, which greatly speeds up the computation process

Regardless the improved implementation and relatively rapid computation of the information flow for any pair of sink and source (0.2-0.3 seconds for a matrix with 25 000 columns and lines), the computation of the total information circulation for the 5000 uniprot cross-referenced proteins is a lengthy task, since it requires the computation of information circulation for 5000^2/2 sink-source pairs. With such implementation and as a single threaded process, this would take over 40 days to compute the organism-wide information flow, without speaking about the GO-specific flows. In order to circumvent it, a  random poll of protein pairs have been performed several times and the information circulation have been made for this pairs of proteins. The procedure was repeated several times and a mean informativity of each term plus standard deviation have been calculated. This allowed to distribute the process and avoid to compute the whole information circulation map. For the GO-term specific information circulation computation, only the exact calculation have been performed for the GO terms covering less then 15 targets. For the GO terms covering more then 15 targets, a random sampling similar to the one performed in hte case of whole proteome information circulation was performed. 

The validity of the metabolome and interactome topologies were tested by verifying if the information transmission for a particular protein decayed on average with the increasing metabolic distance (HiNT data was omited in this case). In fact, [WinterMude2010] have shown that in the bacteria, the metabolic network was able to compensate for the perturbations in the genes separated by more then three steps in the metabolic networks (two physical entities are separated by n steps in the metabolic map if the shortest path between them requires to path only through n other physical entities). This result playing an important factor allowing for the Markov random fiend denoising in graphs to improve the secondary effects models as outlined in the previous chapter. Such a reproduction was successful, see the [figure ???], where to the left you can see the inital experimental result obtained by Wintermude et al. and to the left-the reult of our simulation with (blue) with upper and lower error limit (red and black).

Provided that pure systems biology tools for the estimation of the secondary effects based on the multiple minor perturbations of the function is still a developing field and that the method based on the information circulation have never been used in the drug development context, we've decided to first test the usefullness of this approach for the drug design by using it to retrieve all the potentially good drug target. By this we mean the targets that are likely to infuence a lot the cell functionning and are present in a sufficiently low concentration, so that their inhibition requires sufficiently small amounts of drugs for the off-target effect remain minimal and secondary effects-minor.

> Test for overingtonicity


In addition to that, the module responsible for treatment the GO terms in this module was tested independently on the neflanavir dataset 

> Stub of the GO annotation => test on nelfanavir
Confirmation the results published previously. (Annex V). In short, five clusters criticla for cancer development were damaged simultaneously:
 - Respiration via acetyl-CoA-related processes inhibition
 - cell polarity establishment perturbation => normal cells deal with it easily, but not cnacerous cell where cell division checkpoints are defficient
 - Focal adhesion and anoikis
 - EMT switch differentiation
 - PKB swithc regulation 

 Error estimation have shown that the probability of error for the first 10 terms is lower then 5 %, (pdfs of random trials) (see Annex VI and figure ??? for the enrichement of GO terms compared to a random poll,)

=> Consider GO annotation density to see if the targes in the set of interest are more likely to be decisive for a certain GO process of therapeutical interes.



Problematics: 

  - Fail for the simplicity of use for the protein evolution network

  - Purification of the Network by random matrix theory clustering? [HIV publication, protein evolution publication, ...] Principle: retrieve the random matrix values for a provided random matrix with the standard bounds on random matrix eigenvalues, retrieve the matrix and the eigenvectors to which the largest eigenvectors are associated. Project the matrix into this space and perform the clustering in the linear space generated by those matrixes. => provided high confidence of the source data, this method was omited, yet should definetely be used in less uniform matrixes.

  - Improvement of the original Missiuro algorithm 
	Use of computationally more stable Cholesky decompostion istead of LU decomposition, provided that the resistance matrix is positive definite by construction. Computationally less more stable, even if a small perturbation of the matrix is required in order to get rid of the single positive eigenvalue of the matrix (the sum of all the base vectors). This is usefull from the perspective of inexistence of an overloaded solution within the matrix.

	=> more rapid calculation of the information flow. Isntead of N cholesky decompostions, only one is required. This is needed to make the matrix invertible and actually allow the efficient solution of differential equation

	How much a GO-specific network will be perturbed by the drug off-target effet (provided that given targets are knocked down) => voltage variation at a given informativity flow. However, GO computation is lengthy and is computtionally expensive.

Connexion with the previous method not implemented, but scheduled. Because of the difference in the data format syncing, a direct transfert of the informativity was impossible.

Goals: 
	=> remarks results were attempted to be reproduced as a part of Hatzimanticatis' course, but only 4000 nodes and 8000 relations led to the breakdown of the method 
 - Reproduce Overington's dataset of the protein targets for the approved drugs

=> Alteranative application: use Humsavar to trace back the Human Polymorphism and disease-related mutations onto the informativity table and verify the points of convergence of the perturbations

=> Load the PDB data regarding protein domains and points of protein-protein interaction, project them onto the relevant proteins from the UNIPROT

=> eHiTs protein-protein interaction network.

TODO before presentation
=> verify the list of human essential genes
=> Add the true network-based GO genes circulation to the informativity flow calcylation
=> 

We are not taking in account the EC numbers of enzymes, since they do not point towards the reaction participants, just the catalysing elements.


A convinient database for the DNA transcription / translation regulation? 
epigenetic markers and splicing regulation included?

A ressource for the catalytic RNA roles? regulatory roles of sRNA?

RNA/DNA locations highlights?

Complete the gene ontology with the MedDra terms ontology (which is however way more
suited for sec. effect reporting in medical environement and provides little
biological insight)

Random Matrix theory => equivalent to Ochta's almost neutral evolution theory

Minimal structural clusters paper?

Self-decorrelation required in case of the correlation matrixes, so that we can assume
that the off-diagonal terms are actually obeying a gaussian distribution.

Interest of the new method based on the information circulation for the prediction of medical
activity of a drug against a pathogen hosted by the human immune system.

Random matrix clustering is actually a K-mean on the distances by the first two eigenvectors
=> dafuq, how to interpret this?

Possible method improving: sensor/executer, plus some proteins can be more important then
the others for some functions /survival level. 

Not enough drugs to extensively cover the mechanisms of actions in pathogen and create action models,
like for the secondary effects in humans.

Python lib for biopax parsing: xml.etree from python helps to parse better, even if loads it all
into the RAM (1.2 Gb of ram taken by the Python dict)

neo4j -> persistnence, rapidity and GRAPHICAL INTERFACE!!!! => great for debugging. Integrated
Lucene indexing => Das ist cool, Ja!!!!

Possibility of accounting for the CNS specific functions.
